{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63eac0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ca27ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "\n",
    "    try:\n",
    "        # Outer Tag Object\n",
    "        title = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
    "        \n",
    "        # Inner NavigatableString Object\n",
    "        title_value = title.text\n",
    "\n",
    "        # Title as a string value\n",
    "        title_string = title_value.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "\n",
    "    return title_string\n",
    "\n",
    "# Function to extract Product Price\n",
    "def get_price(soup):\n",
    "\n",
    "    try:\n",
    "        price = soup.find(\"span\", attrs={\"class\":'a-price-whole'}).text\n",
    "\n",
    "    except AttributeError:\n",
    "\n",
    "        try:\n",
    "            # If there is some deal price\n",
    "            price = soup.find(\"span\", attrs={'id':'priceblock_dealprice'}).string.strip()\n",
    "\n",
    "        except:\n",
    "            price = \"\"\n",
    "\n",
    "    return price\n",
    "\n",
    "# Function to extract Product Rating\n",
    "def get_rating(soup):\n",
    "\n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star-mini a-star-mini-3-5 mvt-cm-cr-review-stars-mini'}).string.strip()\n",
    "    \n",
    "    except AttributeError:\n",
    "        try:\n",
    "            rating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "        except:\n",
    "            rating = \"\"\t\n",
    "\n",
    "    return rating\n",
    "\n",
    "# Function to extract Number of User Reviews\n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        review_count = \"\"\t\n",
    "\n",
    "    return review_count\n",
    "\n",
    "# Function to extract Availability Status\n",
    "def get_availability(soup):\n",
    "    try:\n",
    "        available = soup.find(\"div\", attrs={'id':'availability'})\n",
    "        available = available.find(\"span\").string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        available = \"Not Available\"\t\n",
    "\n",
    "    return available\n",
    "\n",
    "# Function to extract Brand\n",
    "def get_brand(soup):\n",
    "    try:\n",
    "        brand = soup.find(\"span\", attrs={\"class\":'a-size-base po-break-word'}).text.strip()\n",
    "    except AttributeError:\n",
    "        \n",
    "        brand = \"Not Available\"\t\n",
    "\n",
    "    return brand\n",
    "\n",
    "# Function to extract Offer\n",
    "def get_off(soup):\n",
    "    try:\n",
    "       off = soup.find(\"span\", attrs={\"class\":\"a-size-large a-color-price savingPriceOverride aok-align-center reinventPriceSavingsPercentageMargin savingsPercentage\"}).text\n",
    "    except AttributeError:\n",
    "        off = \"Not Available\"\n",
    "\n",
    "    return off\n",
    "        \n",
    "\n",
    "def get_bought(soup):\n",
    "    try:\n",
    "        bought=soup.find(\"span\", attrs={\"id\":'social-proofing-faceout-title-tk_bought'}).text.strip()\n",
    "    except AttributeError:\n",
    "        \n",
    "        bought = \"Not Available\"\t\n",
    "\n",
    "    return bought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4713b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Page 1...\n",
      "Scraping Page 2...\n",
      "Scraping Page 3...\n",
      "Scraping Page 4...\n",
      "Total Products Found: 88\n",
      "Scraping product 1 of 88\n",
      "Scraping product 2 of 88\n",
      "Scraping product 3 of 88\n",
      "Scraping product 4 of 88\n",
      "Scraping product 5 of 88\n",
      "Scraping product 6 of 88\n",
      "Scraping product 7 of 88\n",
      "Scraping product 8 of 88\n",
      "Scraping product 9 of 88\n",
      "Scraping product 10 of 88\n",
      "Scraping product 11 of 88\n",
      "Scraping product 12 of 88\n",
      "Scraping product 13 of 88\n",
      "Scraping product 14 of 88\n",
      "Scraping product 15 of 88\n",
      "Scraping product 16 of 88\n",
      "Scraping product 17 of 88\n",
      "Scraping product 18 of 88\n",
      "Scraping product 19 of 88\n",
      "Scraping product 20 of 88\n",
      "Scraping product 21 of 88\n",
      "Scraping product 22 of 88\n",
      "Scraping product 23 of 88\n",
      "Scraping product 24 of 88\n",
      "Scraping product 25 of 88\n",
      "Scraping product 26 of 88\n",
      "Scraping product 27 of 88\n",
      "Scraping product 28 of 88\n",
      "Scraping product 29 of 88\n",
      "Scraping product 30 of 88\n",
      "Scraping product 31 of 88\n",
      "Scraping product 32 of 88\n",
      "Scraping product 33 of 88\n",
      "Scraping product 34 of 88\n",
      "Scraping product 35 of 88\n",
      "Scraping product 36 of 88\n",
      "Scraping product 37 of 88\n",
      "Scraping product 38 of 88\n",
      "Scraping product 39 of 88\n",
      "Scraping product 40 of 88\n",
      "Scraping product 41 of 88\n",
      "Scraping product 42 of 88\n",
      "Scraping product 43 of 88\n",
      "Scraping product 44 of 88\n",
      "Scraping product 45 of 88\n",
      "Scraping product 46 of 88\n",
      "Scraping product 47 of 88\n",
      "Scraping product 48 of 88\n",
      "Scraping product 49 of 88\n",
      "Scraping product 50 of 88\n",
      "Scraping product 51 of 88\n",
      "Scraping product 52 of 88\n",
      "Scraping product 53 of 88\n",
      "Scraping product 54 of 88\n",
      "Scraping product 55 of 88\n",
      "Scraping product 56 of 88\n",
      "Scraping product 57 of 88\n",
      "Scraping product 58 of 88\n",
      "Scraping product 59 of 88\n",
      "Scraping product 60 of 88\n",
      "Scraping product 61 of 88\n",
      "Scraping product 62 of 88\n",
      "Scraping product 63 of 88\n",
      "Scraping product 64 of 88\n",
      "Scraping product 65 of 88\n",
      "Scraping product 66 of 88\n",
      "Scraping product 67 of 88\n",
      "Scraping product 68 of 88\n",
      "Scraping product 69 of 88\n",
      "Scraping product 70 of 88\n",
      "Scraping product 71 of 88\n",
      "Scraping product 72 of 88\n",
      "Scraping product 73 of 88\n",
      "Scraping product 74 of 88\n",
      "Scraping product 75 of 88\n",
      "Scraping product 76 of 88\n",
      "Scraping product 77 of 88\n",
      "Scraping product 78 of 88\n",
      "Scraping product 79 of 88\n",
      "Scraping product 80 of 88\n",
      "Scraping product 81 of 88\n",
      "Scraping product 82 of 88\n",
      "Scraping product 83 of 88\n",
      "Scraping product 84 of 88\n",
      "Scraping product 85 of 88\n",
      "Scraping product 86 of 88\n",
      "Scraping product 87 of 88\n",
      "Scraping product 88 of 88\n",
      "Scraping completed. Data saved to 'amazon_data.csv'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Add your user agent\n",
    "    HEADERS = ({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0',\n",
    "        'Accept-Language': 'en-US, en;q=0.5'\n",
    "    })\n",
    "\n",
    "    # Base URL for pagination\n",
    "    BASE_URL = \"https://www.amazon.in/s?k=laptop&crid=2R49WXP0T6MHX\"\n",
    "\n",
    "    # Number of pages to scrape\n",
    "    NUM_PAGES = 4  \n",
    "\n",
    "    # Store product links\n",
    "    links_list = []\n",
    "\n",
    "    # Loop through multiple pages\n",
    "    for page in range(1, NUM_PAGES + 1):\n",
    "        print(f\"Scraping Page {page}...\")\n",
    "\n",
    "        # Construct paginated URL\n",
    "        url = f\"{BASE_URL}&page={page}\"\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Fetch product links\n",
    "        links = soup.find_all(\"a\", attrs={'class': 'a-link-normal s-line-clamp-2 s-link-style a-text-normal'})\n",
    "        for link in links:\n",
    "            links_list.append(\"https://www.amazon.in\" + link.get('href'))\n",
    "\n",
    "        # Delay to prevent getting blocked\n",
    "        time.sleep(2)\n",
    "\n",
    "    print(f\"Total Products Found: {len(links_list)}\")\n",
    "\n",
    "    # Dictionary to store product data\n",
    "    d = {\"title\": [], \"price\": [], \"rating\": [], \"reviews\": [], \"availability\": [], \"brand\": [], \"category\": [], \"off\": [], \"bought\": []}\n",
    "\n",
    "    # Loop for extracting product details from each link \n",
    "    for idx, link in enumerate(links_list):\n",
    "        print(f\"Scraping product {idx+1} of {len(links_list)}\")\n",
    "\n",
    "        new_webpage = requests.get(link, headers=HEADERS)\n",
    "        new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "        # Append extracted data (ensuring consistent length)\n",
    "        d['title'].append(get_title(new_soup) or \"\")\n",
    "        d['price'].append(get_price(new_soup) or \"\")\n",
    "        d['rating'].append(get_rating(new_soup) or \"\")\n",
    "        d['reviews'].append(get_review_count(new_soup) or \"\")\n",
    "        d['availability'].append(get_availability(new_soup) or \"\")\n",
    "        d['brand'].append(get_brand(new_soup) or \"\")\n",
    "        d['category'].append(\"Laptop\")  # Manually assign 'Laptop' as category\n",
    "        d['off'].append(get_off(new_soup) or \"\")\n",
    "        d['bought'].append(get_bought(new_soup) or \"\")\n",
    "\n",
    "        # Delay between product requests to avoid detection\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    # Save data to CSV\n",
    "    amazon_df = pd.DataFrame.from_dict(d)\n",
    "    amazon_df.to_csv(\"amazon_data.csv\", header=True, index=False)\n",
    "\n",
    "    print(\"Scraping completed. Data saved to 'amazon_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db08fbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviews</th>\n",
       "      <th>availability</th>\n",
       "      <th>brand</th>\n",
       "      <th>category</th>\n",
       "      <th>off</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lenovo V15 G4 AMD Ryzen 5 7520U 15.6 inch FHD ...</td>\n",
       "      <td>34,990.</td>\n",
       "      <td>3.0 out of 5 stars</td>\n",
       "      <td>1 rating</td>\n",
       "      <td>In stock</td>\n",
       "      <td>Lenovo</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>-42%</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apple 2025 MacBook Air (13-inch, Apple M4 chip...</td>\n",
       "      <td>99,900.</td>\n",
       "      <td>Previous page</td>\n",
       "      <td></td>\n",
       "      <td>In stock</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lenovo V15 G3 (2024), Intel Core i3 12th Gen 1...</td>\n",
       "      <td>32,980.</td>\n",
       "      <td>4.2 out of 5 stars</td>\n",
       "      <td>4 ratings</td>\n",
       "      <td>In stock</td>\n",
       "      <td>Lenovo</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>-60%</td>\n",
       "      <td>50+ bought in past month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acer Aspire Lite, AMD Ryzen 5 5625U Processor(...</td>\n",
       "      <td>34,490.</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>1,121 ratings</td>\n",
       "      <td>In stock</td>\n",
       "      <td>acer</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>-42%</td>\n",
       "      <td>1K+ bought in past month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lᥱnovo ThinkPad Touch Screen T490 Laptop Intᥱl...</td>\n",
       "      <td>22,990.</td>\n",
       "      <td>Previous page</td>\n",
       "      <td></td>\n",
       "      <td>In stock</td>\n",
       "      <td>Generic</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>-8%</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Lenovo IdeaPad 3 14 Inch FHD Laptop (12th Gen ...</td>\n",
       "      <td>28,891.</td>\n",
       "      <td>Previous page</td>\n",
       "      <td></td>\n",
       "      <td>Only 1 left in stock.</td>\n",
       "      <td>Lenovo</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Apple 2024 MacBook Pro Laptop with M4 Pro chip...</td>\n",
       "      <td>1,91,990.</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>5 ratings</td>\n",
       "      <td>In stock</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>-4%</td>\n",
       "      <td>50+ bought in past month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Lenovo V15 G4 AMD Athlon Silver 7120U Laptop 8...</td>\n",
       "      <td>24,990.</td>\n",
       "      <td>3.9 out of 5 stars</td>\n",
       "      <td>133 ratings</td>\n",
       "      <td>In stock</td>\n",
       "      <td>Lenovo</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>-34%</td>\n",
       "      <td>50+ bought in past month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Apple 2025 MacBook Air (13-inch, Apple M4 chip...</td>\n",
       "      <td>99,900.</td>\n",
       "      <td>Previous page</td>\n",
       "      <td></td>\n",
       "      <td>In stock</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Apple 2025 MacBook Air (13-inch, Apple M4 chip...</td>\n",
       "      <td>99,900.</td>\n",
       "      <td>4.2 out of 5 stars</td>\n",
       "      <td>4 ratings</td>\n",
       "      <td>Only 1 left in stock.</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title      price  \\\n",
       "0   Lenovo V15 G4 AMD Ryzen 5 7520U 15.6 inch FHD ...    34,990.   \n",
       "1   Apple 2025 MacBook Air (13-inch, Apple M4 chip...    99,900.   \n",
       "2   Lenovo V15 G3 (2024), Intel Core i3 12th Gen 1...    32,980.   \n",
       "3   Acer Aspire Lite, AMD Ryzen 5 5625U Processor(...    34,490.   \n",
       "4   Lᥱnovo ThinkPad Touch Screen T490 Laptop Intᥱl...    22,990.   \n",
       "..                                                ...        ...   \n",
       "83  Lenovo IdeaPad 3 14 Inch FHD Laptop (12th Gen ...    28,891.   \n",
       "84  Apple 2024 MacBook Pro Laptop with M4 Pro chip...  1,91,990.   \n",
       "85  Lenovo V15 G4 AMD Athlon Silver 7120U Laptop 8...    24,990.   \n",
       "86  Apple 2025 MacBook Air (13-inch, Apple M4 chip...    99,900.   \n",
       "87  Apple 2025 MacBook Air (13-inch, Apple M4 chip...    99,900.   \n",
       "\n",
       "                rating        reviews           availability    brand  \\\n",
       "0   3.0 out of 5 stars       1 rating               In stock   Lenovo   \n",
       "1        Previous page                              In stock    Apple   \n",
       "2   4.2 out of 5 stars      4 ratings               In stock   Lenovo   \n",
       "3   4.0 out of 5 stars  1,121 ratings               In stock     acer   \n",
       "4        Previous page                              In stock  Generic   \n",
       "..                 ...            ...                    ...      ...   \n",
       "83       Previous page                 Only 1 left in stock.   Lenovo   \n",
       "84  5.0 out of 5 stars      5 ratings               In stock    Apple   \n",
       "85  3.9 out of 5 stars    133 ratings               In stock   Lenovo   \n",
       "86       Previous page                              In stock    Apple   \n",
       "87  4.2 out of 5 stars      4 ratings  Only 1 left in stock.    Apple   \n",
       "\n",
       "   category            off                    bought  \n",
       "0    Laptop           -42%             Not Available  \n",
       "1    Laptop  Not Available             Not Available  \n",
       "2    Laptop           -60%  50+ bought in past month  \n",
       "3    Laptop           -42%  1K+ bought in past month  \n",
       "4    Laptop            -8%             Not Available  \n",
       "..      ...            ...                       ...  \n",
       "83   Laptop  Not Available             Not Available  \n",
       "84   Laptop            -4%  50+ bought in past month  \n",
       "85   Laptop           -34%  50+ bought in past month  \n",
       "86   Laptop  Not Available             Not Available  \n",
       "87   Laptop  Not Available             Not Available  \n",
       "\n",
       "[88 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b4413e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Page 1...\n",
      "Scraping Page 2...\n",
      "Scraping Page 3...\n",
      "Scraping Page 4...\n",
      "Total Products Found: 88\n",
      "Scraping product 1 of 88\n",
      "Scraping product 2 of 88\n",
      "Scraping product 3 of 88\n",
      "Scraping product 4 of 88\n",
      "Scraping product 5 of 88\n",
      "Scraping product 6 of 88\n",
      "Scraping product 7 of 88\n",
      "Scraping product 8 of 88\n",
      "Scraping product 9 of 88\n",
      "Scraping product 10 of 88\n",
      "Scraping product 11 of 88\n",
      "Scraping product 12 of 88\n",
      "Scraping product 13 of 88\n",
      "Scraping product 14 of 88\n",
      "Scraping product 15 of 88\n",
      "Scraping product 16 of 88\n",
      "Scraping product 17 of 88\n",
      "Scraping product 18 of 88\n",
      "Scraping product 19 of 88\n",
      "Scraping product 20 of 88\n",
      "Scraping product 21 of 88\n",
      "Scraping product 22 of 88\n",
      "Scraping product 23 of 88\n",
      "Scraping product 24 of 88\n",
      "Scraping product 25 of 88\n",
      "Scraping product 26 of 88\n",
      "Scraping product 27 of 88\n",
      "Scraping product 28 of 88\n",
      "Scraping product 29 of 88\n",
      "Scraping product 30 of 88\n",
      "Scraping product 31 of 88\n",
      "Scraping product 32 of 88\n",
      "Scraping product 33 of 88\n",
      "Scraping product 34 of 88\n",
      "Scraping product 35 of 88\n",
      "Scraping product 36 of 88\n",
      "Scraping product 37 of 88\n",
      "Scraping product 38 of 88\n",
      "Scraping product 39 of 88\n",
      "Scraping product 40 of 88\n",
      "Scraping product 41 of 88\n",
      "Scraping product 42 of 88\n",
      "Scraping product 43 of 88\n",
      "Scraping product 44 of 88\n",
      "Scraping product 45 of 88\n",
      "Scraping product 46 of 88\n",
      "Scraping product 47 of 88\n",
      "Scraping product 48 of 88\n",
      "Scraping product 49 of 88\n",
      "Scraping product 50 of 88\n",
      "Scraping product 51 of 88\n",
      "Scraping product 52 of 88\n",
      "Scraping product 53 of 88\n",
      "Scraping product 54 of 88\n",
      "Scraping product 55 of 88\n",
      "Scraping product 56 of 88\n",
      "Scraping product 57 of 88\n",
      "Scraping product 58 of 88\n",
      "Scraping product 59 of 88\n",
      "Scraping product 60 of 88\n",
      "Scraping product 61 of 88\n",
      "Scraping product 62 of 88\n",
      "Scraping product 63 of 88\n",
      "Scraping product 64 of 88\n",
      "Scraping product 65 of 88\n",
      "Scraping product 66 of 88\n",
      "Scraping product 67 of 88\n",
      "Scraping product 68 of 88\n",
      "Scraping product 69 of 88\n",
      "Scraping product 70 of 88\n",
      "Scraping product 71 of 88\n",
      "Scraping product 72 of 88\n",
      "Scraping product 73 of 88\n",
      "Scraping product 74 of 88\n",
      "Scraping product 75 of 88\n",
      "Scraping product 76 of 88\n",
      "Scraping product 77 of 88\n",
      "Scraping product 78 of 88\n",
      "Scraping product 79 of 88\n",
      "Scraping product 80 of 88\n",
      "Scraping product 81 of 88\n",
      "Scraping product 82 of 88\n",
      "Scraping product 83 of 88\n",
      "Scraping product 84 of 88\n",
      "Scraping product 85 of 88\n",
      "Scraping product 86 of 88\n",
      "Scraping product 87 of 88\n",
      "Scraping product 88 of 88\n",
      "Scraping completed. Data saved to 'amazon_data_2.csv'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Add your user agent\n",
    "    HEADERS = ({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0',\n",
    "        'Accept-Language': 'en-US, en;q=0.5'\n",
    "    })\n",
    "\n",
    "    # Base URL for pagination\n",
    "    BASE_URL = \"https://www.amazon.in/s?k=phones&crid=Z3FWW1OLZ58\"\n",
    "\n",
    "    # Number of pages to scrape\n",
    "    NUM_PAGES = 4  \n",
    "\n",
    "    # Store product links\n",
    "    links_list = []\n",
    "\n",
    "    # Loop through multiple pages\n",
    "    for page in range(1, NUM_PAGES + 1):\n",
    "        print(f\"Scraping Page {page}...\")\n",
    "\n",
    "        # Construct paginated URL\n",
    "        url = f\"{BASE_URL}&page={page}\"\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Fetch product links\n",
    "        links = soup.find_all(\"a\", attrs={'class': 'a-link-normal s-line-clamp-2 s-link-style a-text-normal'})\n",
    "        for link in links:\n",
    "            links_list.append(\"https://www.amazon.in\" + link.get('href'))\n",
    "\n",
    "        # Delay to prevent getting blocked\n",
    "        time.sleep(2)\n",
    "\n",
    "    print(f\"Total Products Found: {len(links_list)}\")\n",
    "\n",
    "    # Dictionary to store product data\n",
    "    d = {\"title\": [], \"price\": [], \"rating\": [], \"reviews\": [], \"availability\": [], \"brand\": [], \"category\": [], \"off\": [], \"bought\": []}\n",
    "\n",
    "    # Loop for extracting product details from each link \n",
    "    for idx, link in enumerate(links_list):\n",
    "        print(f\"Scraping product {idx+1} of {len(links_list)}\")\n",
    "\n",
    "        new_webpage = requests.get(link, headers=HEADERS)\n",
    "        new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "        # Append extracted data (ensuring consistent length)\n",
    "        d['title'].append(get_title(new_soup) or \"\")\n",
    "        d['price'].append(get_price(new_soup) or \"\")\n",
    "        d['rating'].append(get_rating(new_soup) or \"\")\n",
    "        d['reviews'].append(get_review_count(new_soup) or \"\")\n",
    "        d['availability'].append(get_availability(new_soup) or \"\")\n",
    "        d['brand'].append(get_brand(new_soup) or \"\")\n",
    "        d['category'].append(\"SmartPhone\")  # Manually assign 'Laptop' as category\n",
    "        d['off'].append(get_off(new_soup) or \"\")\n",
    "        d['bought'].append(get_bought(new_soup) or \"\")\n",
    "\n",
    "        # Delay between product requests to avoid detection\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    # Save data to CSV\n",
    "    amazon_df_2 = pd.DataFrame.from_dict(d)\n",
    "    amazon_df_2.to_csv(\"amazon_data_2.csv\", header=True, index=False)\n",
    "\n",
    "    print(\"Scraping completed. Data saved to 'amazon_data_2.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07d90c04-6335-47d7-907f-c7d5a61836db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Page 1...\n",
      "Scraping Page 2...\n",
      "Scraping Page 3...\n",
      "Scraping Page 4...\n",
      "Total Products Found: 88\n",
      "Scraping product 1 of 88\n",
      "Scraping product 2 of 88\n",
      "Scraping product 3 of 88\n",
      "Scraping product 4 of 88\n",
      "Scraping product 5 of 88\n",
      "Scraping product 6 of 88\n",
      "Scraping product 7 of 88\n",
      "Scraping product 8 of 88\n",
      "Scraping product 9 of 88\n",
      "Scraping product 10 of 88\n",
      "Scraping product 11 of 88\n",
      "Scraping product 12 of 88\n",
      "Scraping product 13 of 88\n",
      "Scraping product 14 of 88\n",
      "Scraping product 15 of 88\n",
      "Scraping product 16 of 88\n",
      "Scraping product 17 of 88\n",
      "Scraping product 18 of 88\n",
      "Scraping product 19 of 88\n",
      "Scraping product 20 of 88\n",
      "Scraping product 21 of 88\n",
      "Scraping product 22 of 88\n",
      "Scraping product 23 of 88\n",
      "Scraping product 24 of 88\n",
      "Scraping product 25 of 88\n",
      "Scraping product 26 of 88\n",
      "Scraping product 27 of 88\n",
      "Scraping product 28 of 88\n",
      "Scraping product 29 of 88\n",
      "Scraping product 30 of 88\n",
      "Scraping product 31 of 88\n",
      "Scraping product 32 of 88\n",
      "Scraping product 33 of 88\n",
      "Scraping product 34 of 88\n",
      "Scraping product 35 of 88\n",
      "Scraping product 36 of 88\n",
      "Scraping product 37 of 88\n",
      "Scraping product 38 of 88\n",
      "Scraping product 39 of 88\n",
      "Scraping product 40 of 88\n",
      "Scraping product 41 of 88\n",
      "Scraping product 42 of 88\n",
      "Scraping product 43 of 88\n",
      "Scraping product 44 of 88\n",
      "Scraping product 45 of 88\n",
      "Scraping product 46 of 88\n",
      "Scraping product 47 of 88\n",
      "Scraping product 48 of 88\n",
      "Scraping product 49 of 88\n",
      "Scraping product 50 of 88\n",
      "Scraping product 51 of 88\n",
      "Scraping product 52 of 88\n",
      "Scraping product 53 of 88\n",
      "Scraping product 54 of 88\n",
      "Scraping product 55 of 88\n",
      "Scraping product 56 of 88\n",
      "Scraping product 57 of 88\n",
      "Scraping product 58 of 88\n",
      "Scraping product 59 of 88\n",
      "Scraping product 60 of 88\n",
      "Scraping product 61 of 88\n",
      "Scraping product 62 of 88\n",
      "Scraping product 63 of 88\n",
      "Scraping product 64 of 88\n",
      "Scraping product 65 of 88\n",
      "Scraping product 66 of 88\n",
      "Scraping product 67 of 88\n",
      "Scraping product 68 of 88\n",
      "Scraping product 69 of 88\n",
      "Scraping product 70 of 88\n",
      "Scraping product 71 of 88\n",
      "Scraping product 72 of 88\n",
      "Scraping product 73 of 88\n",
      "Scraping product 74 of 88\n",
      "Scraping product 75 of 88\n",
      "Scraping product 76 of 88\n",
      "Scraping product 77 of 88\n",
      "Scraping product 78 of 88\n",
      "Scraping product 79 of 88\n",
      "Scraping product 80 of 88\n",
      "Scraping product 81 of 88\n",
      "Scraping product 82 of 88\n",
      "Scraping product 83 of 88\n",
      "Scraping product 84 of 88\n",
      "Scraping product 85 of 88\n",
      "Scraping product 86 of 88\n",
      "Scraping product 87 of 88\n",
      "Scraping product 88 of 88\n",
      "Scraping completed. Data saved to 'amazon_data_3.csv'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Add your user agent\n",
    "    HEADERS = ({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0',\n",
    "        'Accept-Language': 'en-US, en;q=0.5'\n",
    "    })\n",
    "\n",
    "    # Base URL for pagination\n",
    "    BASE_URL = \"https://www.amazon.in/s?k=headphones&crid=39EWCXSJBVWR4\"\n",
    "\n",
    "    # Number of pages to scrape\n",
    "    NUM_PAGES = 4  \n",
    "\n",
    "    # Store product links\n",
    "    links_list = []\n",
    "\n",
    "    # Loop through multiple pages\n",
    "    for page in range(1, NUM_PAGES + 1):\n",
    "        print(f\"Scraping Page {page}...\")\n",
    "\n",
    "        # Construct paginated URL\n",
    "        url = f\"{BASE_URL}&page={page}\"\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Fetch product links\n",
    "        links = soup.find_all(\"a\", attrs={'class': 'a-link-normal s-line-clamp-2 s-link-style a-text-normal'})\n",
    "        for link in links:\n",
    "            links_list.append(\"https://www.amazon.in\" + link.get('href'))\n",
    "\n",
    "        # Delay to prevent getting blocked\n",
    "        time.sleep(2)\n",
    "\n",
    "    print(f\"Total Products Found: {len(links_list)}\")\n",
    "\n",
    "    # Dictionary to store product data\n",
    "    d = {\"title\": [], \"price\": [], \"rating\": [], \"reviews\": [], \"availability\": [], \"brand\": [], \"category\": [], \"off\": [], \"bought\": []}\n",
    "\n",
    "    # Loop for extracting product details from each link \n",
    "    for idx, link in enumerate(links_list):\n",
    "        print(f\"Scraping product {idx+1} of {len(links_list)}\")\n",
    "\n",
    "        new_webpage = requests.get(link, headers=HEADERS)\n",
    "        new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "        # Append extracted data (ensuring consistent length)\n",
    "        d['title'].append(get_title(new_soup) or \"\")\n",
    "        d['price'].append(get_price(new_soup) or \"\")\n",
    "        d['rating'].append(get_rating(new_soup) or \"\")\n",
    "        d['reviews'].append(get_review_count(new_soup) or \"\")\n",
    "        d['availability'].append(get_availability(new_soup) or \"\")\n",
    "        d['brand'].append(get_brand(new_soup) or \"\")\n",
    "        d['category'].append(\"HeadPhones\")  # Manually assign 'Laptop' as category\n",
    "        d['off'].append(get_off(new_soup) or \"\")\n",
    "        d['bought'].append(get_bought(new_soup) or \"\")\n",
    "\n",
    "        # Delay between product requests to avoid detection\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    # Save data to CSV\n",
    "    amazon_df_3 = pd.DataFrame.from_dict(d)\n",
    "    amazon_df_3.to_csv(\"amazon_data_3.csv\", header=True, index=False)\n",
    "\n",
    "    print(\"Scraping completed. Data saved to 'amazon_data_3.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44c39f48-94c7-4711-bd03-2abc958efb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files merged successfully into 'merged_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# List of CSV files to merge\n",
    "csv_files = [\"amazon_data.csv\", \"amazon_data_2.csv\", \"amazon_data_3.csv\"]  # Replace with actual filenames\n",
    "\n",
    "# Read and merge all CSV files\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the merged file\n",
    "merged_df.to_csv(\"tech_gadgets_data.csv\", index=False)\n",
    "\n",
    "print(\"CSV files merged successfully into 'merged_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3d24d-b5bb-47a4-a9af-2c9ad380e05e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
